{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Importing-Libraries:\">Importing Libraries:<a class=\"anchor-link\" href=\"#Importing-Libraries:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Reading-Data:\">Reading Data:<a class=\"anchor-link\" href=\"#Reading-Data:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.read_csv('Churn_Modelling.csv')\n",
    "data.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Finding-Unique-Values:\">Finding Unique Values:<a class=\"anchor-link\" href=\"#Finding-Unique-Values:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"From-the-above,-we-will-not-require-the-first-2-attributes-as-the-are-specific-to-a-customer.-It-is-borderline-with-the-surname-as-this-would-result-to-profiling-so-we-exclude-this-as-well.\">From the above, we will not require the first 2 attributes as the are specific to a customer. It is borderline with the surname as this would result to profiling so we exclude this as well.<a class=\"anchor-link\" href=\"#From-the-above,-we-will-not-require-the-first-2-attributes-as-the-are-specific-to-a-customer.-It-is-borderline-with-the-surname-as-this-would-result-to-profiling-so-we-exclude-this-as-well.\">¶</a></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=data.drop(['RowNumber','CustomerId','Surname'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check variable data types\n",
    "data.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Initial-Data-Analysis:\">Initial Data Analysis:<a class=\"anchor-link\" href=\"#Initial-Data-Analysis:\">¶</a></h2><h3 id=\"Here-our-main-interest-is-to-get-an-understanding-as-to-how-the-given-attributes-relate-too-the-'Exit'-status.\">Here our main interest is to get an understanding as to how the given attributes relate too the 'Exit' status.<a class=\"anchor-link\" href=\"#Here-our-main-interest-is-to-get-an-understanding-as-to-how-the-given-attributes-relate-too-the-'Exit'-status.\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = 'Exited', 'Retained'\n",
    "sizes = [data.Exited[data['Exited']==1].count(), data.Exited[data['Exited']==0].count()]\n",
    "explode = (0, 0.1)\n",
    "colors=['Yellow','Green']\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',colors=colors,\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Percentage of customer Exited and Retained\", size = 15)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"Churned-customers-are-coming-as-20.4%.-So-our-model-needs--to-predict-that-20%-of-the-customers-will-churn.-Given-20%-is-a-small-number,-we-need-to-ensure-that-the-chosen-model-does-predict-with-great-accuracy-this-20%-as-it-is-of-interest-to-the-bank-to-identify-and-keep-this-bunch-as-opposed-to-accurately-predicting-the-customers-that-are-retained.\">Churned customers are coming as 20.4%. So our model needs  to predict that 20% of the customers will churn. Given 20% is a small number, we need to ensure that the chosen model does predict with great accuracy this 20% as it is of interest to the bank to identify and keep this bunch as opposed to accurately predicting the customers that are retained.<a class=\"anchor-link\" href=\"#Churned-customers-are-coming-as-20.4%.-So-our-model-needs--to-predict-that-20%-of-the-customers-will-churn.-Given-20%-is-a-small-number,-we-need-to-ensure-that-the-chosen-model-does-predict-with-great-accuracy-this-20%-as-it-is-of-interest-to-the-bank-to-identify-and-keep-this-bunch-as-opposed-to-accurately-predicting-the-customers-that-are-retained.\">¶</a></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # We first review the 'Status' relation with categorical variables\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(20, 12))\n",
    "sns.countplot(x='Geography', hue = 'Exited',data = data, ax=axarr[0][0])\n",
    "sns.countplot(x='Gender', hue = 'Exited',data = data, ax=axarr[0][1])\n",
    "sns.countplot(x='HasCrCard', hue = 'Exited',data = data, ax=axarr[1][0])\n",
    "sns.countplot(x='IsActiveMember', hue = 'Exited',data = data, ax=axarr[1][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"We-note-the-following-from-our-categorical-variables:\">We note the following from our categorical variables:<a class=\"anchor-link\" href=\"#We-note-the-following-from-our-categorical-variables:\">¶</a></h5><h6 id=\"1.Majority-of-the-data-is-from-persons-from-France.-However,-the-proportion-of-churned-customers-is-with-inversely-related-to-the-population-of-customers-alluding-to-the-bank-possibly-having-a-problem-(maybe-not-enough-customer-service-resources-allocated)-in-the-areas-where-it-has-fewer-clients.\">1.Majority of the data is from persons from France. However, the proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem (maybe not enough customer service resources allocated) in the areas where it has fewer clients.<a class=\"anchor-link\" href=\"#1.Majority-of-the-data-is-from-persons-from-France.-However,-the-proportion-of-churned-customers-is-with-inversely-related-to-the-population-of-customers-alluding-to-the-bank-possibly-having-a-problem-(maybe-not-enough-customer-service-resources-allocated)-in-the-areas-where-it-has-fewer-clients.\">¶</a></h6><h6 id=\"2.The-proportion-of-female-customers-churning-is-also-greater-than-that-of-male-customers.\">2.The proportion of female customers churning is also greater than that of male customers.<a class=\"anchor-link\" href=\"#2.The-proportion-of-female-customers-churning-is-also-greater-than-that-of-male-customers.\">¶</a></h6><h6 id=\"3.Interestingly,-majority-of-the-customers-that-churned-are-those-with-credit-cards.-Given-that-majority-of-the-customers-have-credit-cards-could-prove-this-to-be-just-a-coincidence.\">3.Interestingly, majority of the customers that churned are those with credit cards. Given that majority of the customers have credit cards could prove this to be just a coincidence.<a class=\"anchor-link\" href=\"#3.Interestingly,-majority-of-the-customers-that-churned-are-those-with-credit-cards.-Given-that-majority-of-the-customers-have-credit-cards-could-prove-this-to-be-just-a-coincidence.\">¶</a></h6><h6 id=\"4.Unsurprisingly-the-inactive-members-have-a-greater-churn.-Worryingly-is-that-the-overall-proportion-of-inactive-mebers-is-quite-high-suggesting-that-the-bank-may-need-a-program-implemented-to-turn-this-group-to-active-customers-as-this-will-definately-have-a-positive-impact-on-the-customer-churn.\">4.Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn.<a class=\"anchor-link\" href=\"#4.Unsurprisingly-the-inactive-members-have-a-greater-churn.-Worryingly-is-that-the-overall-proportion-of-inactive-mebers-is-quite-high-suggesting-that-the-bank-may-need-a-program-implemented-to-turn-this-group-to-active-customers-as-this-will-definately-have-a-positive-impact-on-the-customer-churn.\">¶</a></h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axarr = plt.subplots(3,2,figsize=(20, 12))\n",
    "sns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = data, ax=axarr[0][0])\n",
    "sns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = data , ax=axarr[0][1])\n",
    "sns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = data, ax=axarr[1][0])\n",
    "sns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = data, ax=axarr[1][1])\n",
    "sns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = data, ax=axarr[2][0])\n",
    "sns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = data, ax=axarr[2][1])\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"We-note-the-following-from-continuous-data-attributes:\">We note the following from continuous data attributes:<a class=\"anchor-link\" href=\"#We-note-the-following-from-continuous-data-attributes:\">¶</a></h5><h6 id=\"1.There-is-no-significant-difference-in-the-credit-score-distribution-between-retained-and-churned-customers.\">1.There is no significant difference in the credit score distribution between retained and churned customers.<a class=\"anchor-link\" href=\"#1.There-is-no-significant-difference-in-the-credit-score-distribution-between-retained-and-churned-customers.\">¶</a></h6><h6 id=\"2.The-older-customers-are-churning-at-more-than-the-younger-ones-alluding-to-a-difference-in-service-preference-in-the-age-categories.-The-bank-may-need-to-review-their-target-market-or-review-the-strategy-for-retention-between-the-different-age-groups\">2.The older customers are churning at more than the younger ones alluding to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups<a class=\"anchor-link\" href=\"#2.The-older-customers-are-churning-at-more-than-the-younger-ones-alluding-to-a-difference-in-service-preference-in-the-age-categories.-The-bank-may-need-to-review-their-target-market-or-review-the-strategy-for-retention-between-the-different-age-groups\">¶</a></h6><h6 id=\"3.With-regard-to-the-tenure,-the-clients-on-either-extreme-end-(spent-little-time-with-the-bank-or-a-lot-of-time-with-the-bank)-are-more-likely-to-churn-compared-to-those-that-are-of-average-tenure.\">3.With regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.<a class=\"anchor-link\" href=\"#3.With-regard-to-the-tenure,-the-clients-on-either-extreme-end-(spent-little-time-with-the-bank-or-a-lot-of-time-with-the-bank)-are-more-likely-to-churn-compared-to-those-that-are-of-average-tenure.\">¶</a></h6><h6 id=\"4.Worryingly,-the-bank-is-losing-customers-with-significant-bank-balances-which-is-likely-to-hit-their-available-capital-for-lending.\">4.Worryingly, the bank is losing customers with significant bank balances which is likely to hit their available capital for lending.<a class=\"anchor-link\" href=\"#4.Worryingly,-the-bank-is-losing-customers-with-significant-bank-balances-which-is-likely-to-hit-their-available-capital-for-lending.\">¶</a></h6><h6 id=\"5.-Neither-the-product-nor-the-salary-has-a-significant-effect-on-the-likelihood-to-churn.\">5. Neither the product nor the salary has a significant effect on the likelihood to churn.<a class=\"anchor-link\" href=\"#5.-Neither-the-product-nor-the-salary-has-a-significant-effect-on-the-likelihood-to-churn.\">¶</a></h6>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Data-Preparation:\">Data Preparation:<a class=\"anchor-link\" href=\"#Data-Preparation:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Data Preparation:\n",
    "filter1=data[\"Gender\"]==\"Female\"\n",
    "data[\"Gender\"]=filter1*1\n",
    "\n",
    "\n",
    "filter3=data[\"Geography\"]==\"France\"\n",
    "data[\"Is_France\"]=filter3*1\n",
    "filter4=data[\"Geography\"]==\"Spain\"\n",
    "data[\"Is_Spain\"]=filter4*1\n",
    "filter5=data[\"Geography\"]==\"Germany\"\n",
    "data[\"Is_Germany\"]=filter5*1\n",
    "data=data.drop(\"Geography\", axis=1)\n",
    "\n",
    "\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Feature-Engineering-&amp;-Scaling:\">Feature Engineering &amp; Scaling:<a class=\"anchor-link\" href=\"#Feature-Engineering-&amp;-Scaling:\">¶</a></h2><h4 id=\"Adding-features-that-are-likely-to-impact-probability-of-churning-as-we-have-seen-above-in-initial-analytics-part-Balance,-Salary,-Tenure-and-Age-arecreating-impact-on-churning-thus-we-aretrying-to-create-new-features-using-them.\">Adding features that are likely to impact probability of churning as we have seen above in initial analytics part Balance, Salary, Tenure and Age arecreating impact on churning thus we aretrying to create new features using them.<a class=\"anchor-link\" href=\"#Adding-features-that-are-likely-to-impact-probability-of-churning-as-we-have-seen-above-in-initial-analytics-part-Balance,-Salary,-Tenure-and-Age-arecreating-impact-on-churning-thus-we-aretrying-to-create-new-features-using-them.\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Engineering:\n",
    "data['BalanceSalaryRatio'] = data.Balance/data.EstimatedSalary\n",
    "data['TenureByAge'] = data.Tenure/(data.Age)\n",
    "\n",
    "data=data.drop(\"Tenure\", axis=1)\n",
    "data.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Scaling\n",
    "continuous_vars = ['CreditScore',  'Age', 'Balance','NumOfProducts', 'EstimatedSalary', 'BalanceSalaryRatio',\n",
    "                   'TenureByAge']\n",
    "cat_vars = ['HasCrCard', 'IsActiveMember',\"Gender\",\"Is_France\" ,\"Is_Spain\",\"Is_Germany\"]\n",
    "data = data[['Exited'] + continuous_vars + cat_vars]\n",
    "# minMax scaling the continuous variables\n",
    "minVec = data[continuous_vars].min().copy()\n",
    "maxVec = data[continuous_vars].max().copy()\n",
    "data[continuous_vars] = (data[continuous_vars]-minVec)/(maxVec-minVec)\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Feature-Selection:\">Feature Selection:<a class=\"anchor-link\" href=\"#Feature-Selection:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature Selection:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X=data.drop(['Exited'],axis=1)\n",
    "y=data.Exited\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Correlation-Mapping:\">Correlation Mapping:<a class=\"anchor-link\" href=\"#Correlation-Mapping:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Correlation Mapping:\n",
    "corrmat = data.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Spliting-Data:\">Spliting Data:<a class=\"anchor-link\" href=\"#Spliting-Data:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Spliting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x=data.drop(['Exited','Is_Spain','Is_France','Is_Germany','Gender'],axis=1)\n",
    "y=data.Exited\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20,random_state=5)\n",
    "#print(x.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Models:\">Models:<a class=\"anchor-link\" href=\"#Models:\">¶</a></h2><h3 id=\"1.Decision-Tree-Classifier:\">1.Decision Tree Classifier:<a class=\"anchor-link\" href=\"#1.Decision-Tree-Classifier:\">¶</a></h3><h4 id=\"We-did-Hyper-parameter-tuning,-follow-the-results-below:\">We did Hyper parameter tuning, follow the results below:<a class=\"anchor-link\" href=\"#We-did-Hyper-parameter-tuning,-follow-the-results-below:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "samples=[1,2,4,8,16,32,64,128,256,512,1024,2048,4096]\n",
    "predictors = X_train.columns\n",
    "cv_score=[]\n",
    "avg_auc=[]\n",
    "value=[1,3,5,10,25,100,250]\n",
    "model=[]\n",
    "#  to only use the training data here!!\n",
    "for i in range(len(samples)):\n",
    "    for x in range(len(value)):\n",
    "        decision_tree = DecisionTreeClassifier(max_depth=value[x], criterion=\"entropy\",min_samples_leaf=samples[i],random_state=0)\n",
    "        model.append(decision_tree.fit(X_train,Y_train))\n",
    "        avg_auc.append(cross_val_score(decision_tree, X_train, Y_train, cv=10, scoring=\"accuracy\").mean())\n",
    "print(max(avg_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Best-parameter-for-our-Decision-Tree-Classifier:\">Best parameter for our Decision Tree Classifier:<a class=\"anchor-link\" href=\"#Best-parameter-for-our-Decision-Tree-Classifier:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b=avg_auc.index(max(avg_auc))\n",
    "b\n",
    "(model[b])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fitting the model again:\n",
    "decision_tree1 = DecisionTreeClassifier(max_depth=20, criterion=\"entropy\",min_samples_leaf=8,random_state=100)\n",
    "decision_tree1.fit(X_train,Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Finding-Accuracy-on-our-train-data-using-decision-tree-classifier:\">Finding Accuracy on our train data using decision tree classifier:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-train-data-using-decision-tree-classifier:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Predict1 = decision_tree1.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(Predict1, Y_test).ravel()\n",
    "tn, fp, fn, tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = %.5f\" % (metrics.accuracy_score(decision_tree1.predict(X_train), Y_train)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Finding-Accuracy-on-our-test-data-using-decision-tree-classifier:\">Finding Accuracy on our test data using decision tree classifier:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-test-data-using-decision-tree-classifier:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = %.5f\" % (metrics.accuracy_score(decision_tree1.predict(X_test), Y_test)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"2.-Support-Vector-Machine:\">2. Support Vector Machine:<a class=\"anchor-link\" href=\"#2.-Support-Vector-Machine:\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "average_auc=[]\n",
    "\n",
    "Cs = [0.01,0.1,1,10,100]\n",
    "gammas = [0.1, 1, 10, 100]\n",
    "model2=[]\n",
    "#param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "for h in range(len(Cs)):\n",
    "    for k in range(len(gammas)):\n",
    "        svd =svm.SVC(C=Cs[h],gamma=gammas[k])\n",
    "        model2.append(svd.fit(X_train,Y_train))\n",
    "        average_auc.append(cross_val_score(svd, X_train, Y_train, cv=10, scoring=\"accuracy\").mean())\n",
    "#svc_param_selection(X_train,Y_train)    \n",
    "print(max(average_auc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z=average_auc.index(max(average_auc))\n",
    "(model2[z])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Best-parameter-for-our-SVM-model:\">Best parameter for our SVM model:<a class=\"anchor-link\" href=\"#Best-parameter-for-our-SVM-model:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svd =svm.SVC(C=10,gamma=10,probability=True,coef0=10.0)\n",
    "svd.fit(X_train,Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Finding-Accuracy-on-our-train-data-using-Support-Vector-Machine:\">Finding Accuracy on our train data using Support Vector Machine:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-train-data-using-Support-Vector-Machine:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(svd.predict(X_train), Y_train)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"Finding-Accuracy-on-our-test-data-using-Support-Vector-Machine:\">Finding Accuracy on our test data using Support Vector Machine:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-test-data-using-Support-Vector-Machine:\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(svd.predict(X_test), Y_test)) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Predictv = svd.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(Predictv,Y_test ).ravel()\n",
    "tn, fp, fn, tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"3.Random-Forest:\">3.Random Forest:<a class=\"anchor-link\" href=\"#3.Random-Forest:\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#making the instance\n",
    "model=RandomForestClassifier()\n",
    "#hyper parameters set\n",
    "params = {'criterion':['entropy'],\n",
    "          'n_estimators':[40],\n",
    "          'min_samples_leaf':[4],\n",
    "          'min_samples_split':[10], \n",
    "          'random_state':[123],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#learning\n",
    "model1.fit(X_train,Y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"Finding-Accuracy-on-our-train-data-using-Random-Forest:\">Finding Accuracy on our train data using Random Forest:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-train-data-using-Random-Forest:\">¶</a></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(model1.predict(X_train),Y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h5 id=\"Finding-Accuracy-on-our-test-data-using-Random-Forest:\">Finding Accuracy on our test data using Random Forest:<a class=\"anchor-link\" href=\"#Finding-Accuracy-on-our-test-data-using-Random-Forest:\">¶</a></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " print(\"Accuracy:\",metrics.accuracy_score(prediction,Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {'criterion':['entropy'],\n",
    "          'n_estimators':[40],\n",
    "          'min_samples_leaf':[4],\n",
    "          'min_samples_split':[10], \n",
    "          'random_state':[123],\n",
    "          'n_jobs':[-1]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "model1.fit(X_train,Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Predictv,Y_test ).ravel()\n",
    "tn, fp, fn, tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "X = X_train\n",
    "y = Y_train\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"ROC-Curve:\">ROC Curve:<a class=\"anchor-link\" href=\"#ROC-Curve:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=15,12\n",
    "np.random.seed(42)\n",
    "# Remember to use the training data here!! \n",
    "decision_tree = DecisionTreeClassifier(max_depth=5, criterion=\"entropy\",min_samples_leaf=32)\n",
    "decision_tree.fit(X_train,Y_train)\n",
    "svd =svm.SVC(C=50,gamma=1,probability=True)\n",
    "svd.fit(X_train,Y_train)\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "model1.fit(X_train,Y_train)\n",
    "# And to use the test data here!!\n",
    "\n",
    "probs1 = decision_tree.predict_proba(X_test)[:, 1]\n",
    "probs2 =svd.predict_proba(X_test)[:, 1]\n",
    "\n",
    "probs3 = model1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr1, tpr1, thresholds = roc_curve(Y_test, probs1)\n",
    "fpr2, tpr2, thresholds = roc_curve(Y_test, probs2)\n",
    "fpr3, tpr3, thresholds = roc_curve(Y_test, probs3)\n",
    "\n",
    "plt.plot(fpr1, tpr1, label=\"Decision Classifier\")\n",
    "plt.plot(fpr2, tpr2, label=\"SVM\")\n",
    "plt.plot(fpr3, tpr3, label=\"RFC\")\n",
    "plt.plot([0,1],[0,1],'k-')\n",
    "\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# predicted probabilities generated by sklearn classifier\n",
    "skplt.metrics.plot_roc_curve(prob11,Y_test)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Error-Matrics:\">Error Matrics:<a class=\"anchor-link\" href=\"#Error-Matrics:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Confusion-Matrics:\">Confusion Matrics:<a class=\"anchor-link\" href=\"#Confusion-Matrics:\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test,probs10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs10 = decision_tree.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(rf_random.predict(X_test))) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_random.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prob_new=rf_random.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({\"Y_test\": Y_test,\"prob_new\": prob_new})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.sort_values(\"prob_new\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"hitesh.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(row):\n",
    "    if row[\"prob_new\"]>=0.7:\n",
    "        val=1\n",
    "    else:\n",
    "        val=0\n",
    "    return val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"label7\"]= df.apply(f,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.sort_values(\"prob_new\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label8))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label7))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label6))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label5))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label4))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"Accuracy = %.3f\" % (metrics.accuracy_score(df.Y_test,df.label3))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=15,12\n",
    "np.random.seed(42)\n",
    "fpr1, tpr1, thresholds = roc_curve(df.Y_test,df.label5)\n",
    "plt.plot(fpr1, tpr1, label=\"RFE\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics.roc_curve(df.Y_test,df.label5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix(df.Y_test,df.label5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n",
    "                    {'kernel': ['sigmoid'], 'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                     'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}\n",
    "                   ]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    svc = svm.SVC(gamma=\"scale\")\n",
    "    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train,Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#making the instance\n",
    "model=RandomForestClassifier()\n",
    "#hyper parameters set\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[10,15,20,25,30],\n",
    "          'min_samples_leaf':[1,2,3],\n",
    "          'min_samples_split':[3,4,5,6,7], \n",
    "          'random_state':[123],\n",
    "          'n_jobs':[-1]}\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
    "#learning\n",
    "model1.fit(X_train,Y_train)\n",
    "#The best hyper parameters set\n",
    "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
    "#Prediction\n",
    "prediction=model1.predict(X_test)\n",
    "#importing the metrics module\n",
    "from sklearn import metrics\n",
    "#evaluation(Accuracy)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(prediction,Y_test))\n",
    "#evaluation(Confusion Metrix)\n",
    "print(\"Confusion Metrix:\\n\",metrics.confusion_matrix(prediction,Y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
